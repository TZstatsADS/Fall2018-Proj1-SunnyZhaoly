X_train<-Xtrain[-((1+(j-1)*fold):(j*fold)),]
Y_train<-Ytrain[-((1+(j-1)*fold):(j*fold))]
Wtrain<-w[-((1+(j-1)*fold):(j*fold))]
cv_pars<-train(X_train, Wtrain, Y_train)
Xtrain_pred<-classify(X_train, cv_pars)
train_error<-sum(Wtrain*(Xtrain_pred!=Y_train))/sum(Wtrain)
cv_alpha<-log((1-train_error)/train_error)
cv_lable<-agg_class(x_test, cv_alpha, cv_pars)
dim(cv_pars)
#cv_label <- agg_class(x_test, c(alpha[0:(b-1),], cv_alpha), rbind(allPars[0:(b-1),], cv_pars))
# compute cv error rate
cv_error <- sum(cv_lable != x_test) / fold
cv_errors[j,]<-cv_error
#test_error<-sum(cv_lable!=y_test)/fold
}
cv_avg_error<-mean(cv_errors)
return(cv_avg_error)
}
AdaBoost<-function(Xtrain, Ytrain, B, Xtest,Ytest){
n<-nrow(Xtrain)
alpha<-matrix(0,B,1)
w<-matrix(1/n,n,1)
pars<-matrix(0,B,3)
errors<-matrix(0,B,3)
error<-matrix(0,B,1)
lable<-matrix(0,B)
for (b in 1:B){
cv_error[b]<-cv(Xtrain, Ytrain, k=5, pars, alpha, b,w)
#cv_error[k]<-cv(Xtrain, Ytrain, k, pars, alpha, w)
pars[b,]<-train(Xtrain, w, Ytrain)
lable[b]<-classify(Xtrain, pars[b,])
error[b]<-sum(w*(Ytrain!=lable[b]))/sum(w)
alpha[b]<-log((1-error[b])/error[b])
w<-w*exp(alpha[b]*(Ytrain!=lable[b]))
#test
test_labels<-agg_class(Xtest, alpha[1:b], pars[1:b,])
test_error<-sum(test_labels!=Ytest)/nrow(Ytest)
errors[b,1]<-error[b]
errors[b,2]<-cv_error[b]
errors[b,3]<-test_error[b]
}
return(errors)
}
AdaBoost(Xtrain, Ytrain, B=50,Xtest, Ytest)
rm(list=ls())
train3<-read.table("train_3.txt",sep = ",")
train8<-read.table("train_8.txt",sep = ",")
Xtrain<-rbind(train3,train8)
Ytrain<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#data<-cbind(Y,rbind(train3,train8))
test<-read.table("zip_test.txt",sep = " ")
Xtest<-rbind(test[test$V1==3,-1],test[test$V1==8,-1])
Ytest<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#1.three functions
library(freestats)
train<-function(X,w,y){
#X is training matrix, w is weights, y is class lables
pars<-decisionStump(X,w,y)
return(c(pars$j, pars$theta, pars$m))
}
classify<-function(X,pars){
m<-pars[3]
j<-pars[1]
theta<-pars[2]
classfied<-rep(-m,nrow(X))
x_d<-X[,j]
classfied[x_d>theta]<-m
lable<-matrix(classfied)
return(lable)
}
agg_class <- function(X, alpha, allPars) {
# evaluates the boosting classifier on X
# return the classified result with shape n x 1
n <- nrow(X)
B <- length(alpha)
agg_labels <- matrix(0, n, 1)
if (B == 1) {
# deal with the case when there is one row, the matrix indexing is no longer applicable
allPars <- rbind(allPars, matrix(0,1,3))
}
for (i in seq(B)) {
a <- alpha[i]
agg_labels <- agg_labels + a * classify(X, allPars[i,])
}
classified <- matrix(-1, n, 1)
classified[agg_labels >= 0] <- 1
return(classified)
}
#3
cv<-function(Xtrain, Ytrain, k=5,allPars,alpha,b,w){
n<-nrow(Xtrain)
fold<-n/k
cv_errors<-matrix(0,5,1)
#cv_errors<-matrix(0,5,1)
for (j in 1:k){
x_test<-Xtrain[((1+(j-1)*fold):(j*fold)),]
y_test<-Ytrain[((1+(j-1)*fold):(j*fold))]
X_train<-Xtrain[-((1+(j-1)*fold):(j*fold)),]
Y_train<-Ytrain[-((1+(j-1)*fold):(j*fold))]
Wtrain<-w[-((1+(j-1)*fold):(j*fold))]
cv_pars<-train(X_train, Wtrain, Y_train)
Xtrain_pred<-classify(X_train, cv_pars)
train_error<-sum(Wtrain*(Xtrain_pred!=Y_train))/sum(Wtrain)
cv_alpha<-log((1-train_error)/train_error)
cv_lable<-agg_class(x_test, cv_alpha, cv_pars)
dim(cv_pars)
#cv_label <- agg_class(x_test, c(alpha[0:(b-1),], cv_alpha), rbind(allPars[0:(b-1),], cv_pars))
# compute cv error rate
cv_error <- sum(cv_lable != x_test) / fold
cv_errors[j,]<-cv_error
#test_error<-sum(cv_lable!=y_test)/fold
}
cv_avg_error<-mean(cv_errors)
return(cv_avg_error)
}
AdaBoost<-function(Xtrain, Ytrain, B, Xtest,Ytest){
n<-nrow(Xtrain)
alpha<-matrix(0,B,1)
w<-matrix(1/n,n,1)
pars<-matrix(0,B,3)
errors<-matrix(0,B,3)
error<-matrix(0,B,1)
lable<-matrix(0,B)
for (b in 1:B){
cv_error[b]<-cv(Xtrain, Ytrain, k=5, pars, alpha, b,w)
#cv_error[k]<-cv(Xtrain, Ytrain, k, pars, alpha, w)
pars[b,]<-train(Xtrain, w, Ytrain)
lable[b]<-classify(Xtrain, pars[b,])
error[b]<-sum(w*(Ytrain!=lable[b]))/sum(w)
alpha[b]<-log((1-error[b])/error[b])
w<-w*exp(alpha[b]*(Ytrain!=lable[b]))
#test
test_labels<-agg_class(Xtest, alpha[1:b], pars[1:b,])
test_error<-sum(test_labels!=Ytest)/nrow(Ytest)
errors[b,1]<-error[b]
errors[b,2]<-cv_error[b]
errors[b,3]<-test_error[b]
}
return(errors)
}
AdaBoost(Xtrain, Ytrain, B=50,Xtest, Ytest)
rm(list=ls())
train3<-read.table("train_3.txt",sep = ",")
train8<-read.table("train_8.txt",sep = ",")
Xtrain<-rbind(train3,train8)
Ytrain<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#data<-cbind(Y,rbind(train3,train8))
test<-read.table("zip_test.txt",sep = " ")
Xtest<-rbind(test[test$V1==3,-1],test[test$V1==8,-1])
Ytest<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#1.three functions
library(freestats)
train<-function(X,w,y){
#X is training matrix, w is weights, y is class lables
pars<-decisionStump(X,w,y)
return(c(pars$j, pars$theta, pars$m))
}
classify<-function(X,pars){
m<-pars[3]
j<-pars[1]
theta<-pars[2]
classfied<-rep(-m,nrow(X))
x_d<-X[,j]
classfied[x_d>theta]<-m
lable<-matrix(classfied)
return(lable)
}
agg_class <- function(X, alpha, allPars) {
# evaluates the boosting classifier on X
# return the classified result with shape n x 1
n <- nrow(X)
B <- length(alpha)
agg_labels <- matrix(0, n, 1)
if (B == 1) {
# deal with the case when there is one row, the matrix indexing is no longer applicable
allPars <- rbind(allPars, matrix(0,1,3))
}
for (i in seq(B)) {
a <- alpha[i]
agg_labels <- agg_labels + a * classify(X, allPars[i,])
}
classified <- matrix(-1, n, 1)
classified[agg_labels >= 0] <- 1
return(classified)
}
#3
cv<-function(Xtrain, Ytrain, k=5,allPars,alpha,b,w){
n<-nrow(Xtrain)
fold<-n/k
cv_errors<-matrix(0,5,1)
#cv_errors<-matrix(0,5,1)
for (j in 1:k){
x_test<-Xtrain[((1+(j-1)*fold):(j*fold)),]
y_test<-Ytrain[((1+(j-1)*fold):(j*fold))]
X_train<-Xtrain[-((1+(j-1)*fold):(j*fold)),]
Y_train<-Ytrain[-((1+(j-1)*fold):(j*fold))]
Wtrain<-w[-((1+(j-1)*fold):(j*fold))]
cv_pars<-train(X_train, Wtrain, Y_train)
Xtrain_pred<-classify(X_train, cv_pars)
train_error<-sum(Wtrain*(Xtrain_pred!=Y_train))/sum(Wtrain)
cv_alpha<-log((1-train_error)/train_error)
cv_lable<-agg_class(x_test, cv_alpha, cv_pars)
dim(cv_pars)
#cv_label <- agg_class(x_test, c(alpha[0:(b-1),], cv_alpha), rbind(allPars[0:(b-1),], cv_pars))
# compute cv error rate
cv_error <- sum(cv_lable != x_test) / fold
cv_errors[j,]<-cv_error
#test_error<-sum(cv_lable!=y_test)/fold
}
cv_avg_error<-mean(cv_errors)
return(cv_avg_error)
}
AdaBoost<-function(Xtrain, Ytrain, B, Xtest,Ytest){
n<-nrow(Xtrain)
alpha<-matrix(0,B,1)
w<-matrix(1/n,n,1)
pars<-matrix(0,B,3)
errors<-matrix(0,B,3)
error<-matrix(0,B,1)
lable<-matrix(0,B)
cv_error<-matrix(0,B)
for (b in 1:B){
cv_error[b]<-cv(Xtrain, Ytrain, k=5, pars, alpha, b,w)
#cv_error[k]<-cv(Xtrain, Ytrain, k, pars, alpha, w)
pars[b,]<-train(Xtrain, w, Ytrain)
lable[b]<-classify(Xtrain, pars[b,])
error[b]<-sum(w*(Ytrain!=lable[b]))/sum(w)
alpha[b]<-log((1-error[b])/error[b])
w<-w*exp(alpha[b]*(Ytrain!=lable[b]))
#test
test_labels<-agg_class(Xtest, alpha[1:b], pars[1:b,])
test_error<-sum(test_labels!=Ytest)/nrow(Ytest)
errors[b,1]<-error[b]
errors[b,2]<-cv_error[b]
errors[b,3]<-test_error[b]
}
return(errors)
}
AdaBoost(Xtrain, Ytrain, B=50,Xtest, Ytest)
rm(list=ls())
train3<-read.table("train_3.txt",sep = ",")
train8<-read.table("train_8.txt",sep = ",")
Xtrain<-rbind(train3,train8)
Ytrain<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#data<-cbind(Y,rbind(train3,train8))
test<-read.table("zip_test.txt",sep = " ")
Xtest<-rbind(test[test$V1==3,-1],test[test$V1==8,-1])
Ytest<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#1.three functions
library(freestats)
train<-function(X,w,y){
#X is training matrix, w is weights, y is class lables
pars<-decisionStump(X,w,y)
return(c(pars$j, pars$theta, pars$m))
}
classify<-function(X,pars){
m<-pars[3]
j<-pars[1]
theta<-pars[2]
classfied<-rep(-m,nrow(X))
x_d<-X[,j]
classfied[x_d>theta]<-m
lable<-matrix(classfied)
return(lable)
}
agg_class <- function(X, alpha, allPars) {
# evaluates the boosting classifier on X
# return the classified result with shape n x 1
n <- nrow(X)
B <- length(alpha)
agg_labels <- matrix(0, n, 1)
if (B == 1) {
# deal with the case when there is one row, the matrix indexing is no longer applicable
allPars <- rbind(allPars, matrix(0,1,3))
}
for (i in seq(B)) {
a <- alpha[i]
agg_labels <- agg_labels + a * classify(X, allPars[i,])
}
classified <- matrix(-1, n, 1)
classified[agg_labels >= 0] <- 1
return(classified)
}
#3
cv<-function(Xtrain, Ytrain, k=5,allPars,alpha,b,w){
n<-nrow(Xtrain)
fold<-n/k
cv_errors<-matrix(0,5,1)
#cv_errors<-matrix(0,5,1)
for (j in 1:k){
x_test<-Xtrain[((1+(j-1)*fold):(j*fold)),]
y_test<-Ytrain[((1+(j-1)*fold):(j*fold))]
X_train<-Xtrain[-((1+(j-1)*fold):(j*fold)),]
Y_train<-Ytrain[-((1+(j-1)*fold):(j*fold))]
Wtrain<-w[-((1+(j-1)*fold):(j*fold))]
cv_pars<-train(X_train, Wtrain, Y_train)
Xtrain_pred<-classify(X_train, cv_pars)
train_error<-sum(Wtrain*(Xtrain_pred!=Y_train))/sum(Wtrain)
cv_alpha<-log((1-train_error)/train_error)
cv_lable<-agg_class(x_test, cv_alpha, cv_pars)
dim(cv_pars)
#cv_label <- agg_class(x_test, c(alpha[0:(b-1),], cv_alpha), rbind(allPars[0:(b-1),], cv_pars))
# compute cv error rate
cv_error <- sum(cv_lable != x_test) / fold
cv_errors[j,]<-cv_error
#test_error<-sum(cv_lable!=y_test)/fold
}
cv_avg_error<-mean(cv_errors)
return(cv_avg_error)
}
AdaBoost<-function(Xtrain, Ytrain, B, Xtest,Ytest){
n<-nrow(Xtrain)
alpha<-matrix(0,B,1)
w<-matrix(1/n,n,1)
pars<-matrix(0,B,3)
errors<-matrix(0,B,3)
error<-matrix(0,B,1)
lable<-matrix(0,B)
cv_error<-matrix(0,B)
for (b in 1:B){
cv_error[b]<-cv(Xtrain, Ytrain, k=5, pars, alpha, b,w)
#cv_error[k]<-cv(Xtrain, Ytrain, k, pars, alpha, w)
pars[b,]<-train(Xtrain, w, Ytrain)
lable[b]<-classify(Xtrain, pars[b,])
error[b]<-sum(w*(Ytrain!=lable[b]))/sum(w)
alpha[b]<-log((1-error[b])/error[b])
w<-w*exp(alpha[b]*(Ytrain!=lable[b]))
#test
test_labels<-agg_class(Xtest, alpha[1:b], pars[1:b,])
test_error<-sum(test_labels!=Ytest[1:b])/nrow(Ytest[1:b])
errors[b,1]<-error[b]
errors[b,2]<-cv_error[b]
errors[b,3]<-test_error[b]
}
return(errors)
}
AdaBoost(Xtrain, Ytrain, B=50,Xtest, Ytest)
rm(list=ls())
train3<-read.table("train_3.txt",sep = ",")
train8<-read.table("train_8.txt",sep = ",")
Xtrain<-rbind(train3,train8)
Ytrain<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#data<-cbind(Y,rbind(train3,train8))
test<-read.table("zip_test.txt",sep = " ")
Xtest<-rbind(test[test$V1==3,-1],test[test$V1==8,-1])
Ytest<-rbind(matrix(1,nrow(train3),1),matrix(-1,nrow(train8),1))
#1.three functions
library(freestats)
train<-function(X,w,y){
#X is training matrix, w is weights, y is class lables
pars<-decisionStump(X,w,y)
return(c(pars$j, pars$theta, pars$m))
}
classify<-function(X,pars){
m<-pars[3]
j<-pars[1]
theta<-pars[2]
classfied<-rep(-m,nrow(X))
x_d<-X[,j]
classfied[x_d>theta]<-m
lable<-matrix(classfied)
return(lable)
}
agg_class <- function(X, alpha, allPars) {
# evaluates the boosting classifier on X
# return the classified result with shape n x 1
n <- nrow(X)
B <- length(alpha)
agg_labels <- matrix(0, n, 1)
if (B == 1) {
# deal with the case when there is one row, the matrix indexing is no longer applicable
allPars <- rbind(allPars, matrix(0,1,3))
}
for (i in seq(B)) {
a <- alpha[i]
agg_labels <- agg_labels + a * classify(X, allPars[i,])
}
classified <- matrix(-1, n, 1)
classified[agg_labels >= 0] <- 1
return(classified)
}
#3
cv<-function(Xtrain, Ytrain, k=5,allPars,alpha,b,w){
n<-nrow(Xtrain)
fold<-n/k
cv_errors<-matrix(0,5,1)
#cv_errors<-matrix(0,5,1)
for (j in 1:k){
x_test<-Xtrain[((1+(j-1)*fold):(j*fold)),]
y_test<-Ytrain[((1+(j-1)*fold):(j*fold))]
X_train<-Xtrain[-((1+(j-1)*fold):(j*fold)),]
Y_train<-Ytrain[-((1+(j-1)*fold):(j*fold))]
Wtrain<-w[-((1+(j-1)*fold):(j*fold))]
cv_pars<-train(X_train, Wtrain, Y_train)
Xtrain_pred<-classify(X_train, cv_pars)
train_error<-sum(Wtrain*(Xtrain_pred!=Y_train))/sum(Wtrain)
cv_alpha<-log((1-train_error)/train_error)
cv_lable<-agg_class(x_test, cv_alpha, cv_pars)
dim(cv_pars)
#cv_label <- agg_class(x_test, c(alpha[0:(b-1),], cv_alpha), rbind(allPars[0:(b-1),], cv_pars))
# compute cv error rate
cv_error <- sum(cv_lable != x_test) / fold
cv_errors[j,]<-cv_error
#test_error<-sum(cv_lable!=y_test)/fold
}
cv_avg_error<-mean(cv_errors)
return(cv_avg_error)
}
AdaBoost<-function(Xtrain, Ytrain, B, Xtest,Ytest){
n<-nrow(Xtrain)
alpha<-matrix(0,B,1)
w<-matrix(1/n,n,1)
pars<-matrix(0,B,3)
errors<-matrix(0,B,3)
error<-matrix(0,B,1)
lable<-matrix(0,B)
cv_error<-matrix(0,B)
for (b in 1:B){
cv_error[b]<-cv(Xtrain, Ytrain, k=5, pars, alpha, b,w)
#cv_error[k]<-cv(Xtrain, Ytrain, k, pars, alpha, w)
pars[b,]<-train(Xtrain, w, Ytrain)
lable[b]<-classify(Xtrain, pars[b,])
error[b]<-sum(w*(Ytrain!=lable[b]))/sum(w)
alpha[b]<-log((1-error[b])/error[b])
w<-w*exp(alpha[b]*(Ytrain!=lable[b]))
#test
test_labels<-agg_class(Xtest, alpha[1:b], pars[1:b,])
test_error<-sum(test_labels!=Ytest[1:b])/nrow(Ytest[1:b])
errors[b,1]<-error[b]
errors[b,2]<-cv_error[b]
errors[b,3]<-test_error[b]
}
return(errors)
}
AdaBoost(Xtrain, Ytrain, B=50,Xtest, Ytest)
shiny::runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
shiny::runApp('app')
runApp('app')
runApp('app')
runApp('app')
test[,test$V7>1]
test[,test$V7>0]
test[test[,test$V7]>0]
test[,test$V7]>0
test[1,test$V7]>0
row.names(test)
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
matrix(c(1,2,3),c(3,2,1))
runApp('app')
runApp('app')
install.packages('rsconnect')
rsconnect::setAccountInfo(name='lingyizhao',
token='D7FBB00B8A4255DD5836335CF50D0289',
secret='<SECRET>')
rsconnect::setAccountInfo(name='lingyizhao',
token='D7FBB00B8A4255DD5836335CF50D0289',
secret='<SECRET>')
rsconnect::setAccountInfo(name='lingyizhao', token='D7FBB00B8A4255DD5836335CF50D0289', secret='iJv1uXuaFXeN+YbBs1M7dwfYDcLml/HF/Rd/y6j8')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
getwd()
rsconnect::deployApp('/Users/SunnyZhao/Desktop/app')
runApp('app')
runApp('app')
shiny::runApp('app')
runApp('app')
knitr::opts_chunk$set(echo = TRUE)
install.packages("tm")
install.packages("tidyverse")
install.packages("tidytext")
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
install.packages("tidytext")
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
install.packages("DT")
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
